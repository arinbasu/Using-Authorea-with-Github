As can be seen in the above figure, Type I error occurs when we falsely reject the null hypothesis, and Type II error occurs when falsely fail to reject the null hypothesis. This is referred to as alpha error and this is how we control for the play of chance. Imagine you are conducting a study on the association of second hand environmental tobacco smoke and lung cancer. Your theory may be that compared with those who do not have lung cancer, those who have lung cancer may have higher level of exposure to environmental tobacco smoke. A counter argument to your theory (that environmental tobacco smoke causes lung cancer) and specific hypothesis (that if we were to compare the environmental tobacco smoke measurements, we would find the levels higher on average among those who have suffered from lung cancer than those who weren't all other things being equal) would be that the levels of ETS would be no more higher among those with lung cancer than those without and that, even if you were to find such an association, that association would be a chance finding. The way to rule out whether what you found was a chance finding would be to specify beforehand that chance can explain the findings but also specify the level of that chance. By convention, this is set at 5%. Then, after you complete data collection and analysis, you test what is the probability that what you found could be explained by chance alone or under the conditions of the null. This value is referred to as p-value. Again, as with the alpha error, the p-value is set at 5% level. However, do not use the p-value as  